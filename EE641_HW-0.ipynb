{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9abf90-cd34-4c31-9e50-2a542de8e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6c671c-029e-4f9e-b406-a3c8f1e9428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block component for building ResNet architecture\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, input_channels, output_channels, stride=1, downsample_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # First convolution layer\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(output_channels)\n",
    "        \n",
    "        # Second convolution layer\n",
    "        self.conv2 = nn.Conv2d(output_channels, output_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(output_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample_layer = downsample_layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual_connection = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample_layer is not None:\n",
    "            residual_connection = self.downsample_layer(x)\n",
    "            \n",
    "        out += residual_connection\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53067b2-d956-4bc8-b5d7-dfb3446d0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    \"\"\"Custom ResNet implementation from scratch for CIFAR-100 classification\"\"\"\n",
    "    \n",
    "    def __init__(self, residual_block, layer_config, num_classes=100, dropout_rate=0.0):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        \n",
    "        self.current_channels = 32\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Initial convolution layer - optimized for CIFAR-100 (32x32 images)\n",
    "        self.initial_conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.initial_bn = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Residual layer groups\n",
    "        self.conv2_group = self._build_residual_layer(residual_block, 32, layer_config[0], stride=1)   # conv2_x\n",
    "        self.conv3_group = self._build_residual_layer(residual_block, 64, layer_config[1], stride=2)   # conv3_x\n",
    "        self.conv4_group = self._build_residual_layer(residual_block, 128, layer_config[2], stride=2)  # conv4_x\n",
    "        self.conv5_group = self._build_residual_layer(residual_block, 256, layer_config[3], stride=2)  # conv5_x\n",
    "        \n",
    "        # Global average pooling and final classifier\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "        self.final_classifier = nn.Linear(256 * residual_block.expansion, num_classes)\n",
    "        \n",
    "        # Initialize network weights\n",
    "        self._initialize_network_weights()\n",
    "        \n",
    "    def _build_residual_layer(self, residual_block, output_channels, num_blocks, stride=1):\n",
    "        downsample_layer = None\n",
    "        \n",
    "        # Create downsample layer if dimensions change\n",
    "        if stride != 1 or self.current_channels != output_channels * residual_block.expansion:\n",
    "            downsample_layer = nn.Sequential(\n",
    "                nn.Conv2d(self.current_channels, output_channels * residual_block.expansion,\n",
    "                         kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(output_channels * residual_block.expansion),\n",
    "            )\n",
    "        \n",
    "        block_layers = []\n",
    "        block_layers.append(residual_block(self.current_channels, output_channels, stride, downsample_layer))\n",
    "        self.current_channels = output_channels * residual_block.expansion\n",
    "        \n",
    "        for _ in range(1, num_blocks):\n",
    "            block_layers.append(residual_block(self.current_channels, output_channels))\n",
    "            \n",
    "        return nn.Sequential(*block_layers)\n",
    "    \n",
    "    def _initialize_network_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial convolution and normalization\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.initial_bn(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Pass through residual layer groups\n",
    "        x = self.conv2_group(x)  # 32x32 -> 32x32\n",
    "        x = self.conv3_group(x)  # 32x32 -> 16x16\n",
    "        x = self.conv4_group(x)  # 16x16 -> 8x8\n",
    "        x = self.conv5_group(x)  # 8x8 -> 4x4\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)  # 4x4 -> 1x1\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Apply dropout if specified\n",
    "        if self.dropout_rate > 0:\n",
    "            x = self.dropout_layer(x)\n",
    "            \n",
    "        # Final classification\n",
    "        x = self.final_classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26308b35-0b14-4c2d-80ca-2e556b3866b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_resnet(layer_structure=[2, 4, 4, 2], num_classes=100, dropout_rate=0.0):\n",
    "    \"\"\"Build custom ResNet with specified layer configuration\"\"\"\n",
    "    return CustomResNet(ResidualBlock, layer_structure, num_classes, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4395041e-2d20-4bed-bcaf-ac152721dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_layer_output_dimensions():\n",
    "    \"\"\"Calculate and return output dimensions for each convolution group\"\"\"\n",
    "    # For CIFAR-100 input images (32x32 pixels)\n",
    "    layer_dimensions = {\n",
    "        'input_image': (3, 32, 32),\n",
    "        'initial_conv': (32, 32, 32),\n",
    "        'conv2_group': (32, 32, 32),  # conv2_x\n",
    "        'conv3_group': (64, 16, 16),  # conv3_x\n",
    "        'conv4_group': (128, 8, 8),   # conv4_x  \n",
    "        'conv5_group': (256, 4, 4),   # conv5_x\n",
    "        'global_avg_pool': (256, 1, 1),\n",
    "        'final_classifier': (100,)\n",
    "    }\n",
    "    return layer_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90850130-a0f9-4671-b7c6-d3a1b7a5d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTrainer:\n",
    "    \"\"\"Main trainer class for CIFAR-100 ResNet experiments\"\"\"\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def prepare_cifar100_data(self, batch_size=128, validation_ratio=0.1):\n",
    "        \"\"\"Prepare CIFAR-100 data loaders with augmentation and train/val/test splits\"\"\"\n",
    "        \n",
    "        # Data augmentation transforms for training\n",
    "        training_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], \n",
    "                               std=[0.2675, 0.2565, 0.2761])\n",
    "        ])\n",
    "        \n",
    "        # Simple transforms for validation/testing\n",
    "        evaluation_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], \n",
    "                               std=[0.2675, 0.2565, 0.2761])\n",
    "        ])\n",
    "        \n",
    "        # Load CIFAR-100 datasets\n",
    "        complete_training_set = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=True, download=True, transform=training_transforms)\n",
    "        testing_set = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=False, download=True, transform=evaluation_transforms)\n",
    "        \n",
    "        # Split training data into train/validation\n",
    "        train_size = int((1 - validation_ratio) * len(complete_training_set))\n",
    "        val_size = len(complete_training_set) - train_size\n",
    "        training_subset, validation_subset = random_split(complete_training_set, [train_size, val_size])\n",
    "        \n",
    "        # Create validation set with evaluation transforms\n",
    "        validation_subset.dataset = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=True, download=False, transform=evaluation_transforms)\n",
    "        val_indices = validation_subset.indices\n",
    "        validation_subset = torch.utils.data.Subset(validation_subset.dataset, val_indices)\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                     shuffle=True, num_workers=2, pin_memory=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                                   shuffle=False, num_workers=2, pin_memory=True)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                                    shuffle=False, num_workers=2, pin_memory=True)\n",
    "        \n",
    "        print(f\"Train samples: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "        print(f\"Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "    \n",
    "    def train_epoch(self, model, optimizer, criterion):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/len(pbar):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate_epoch(self, model, criterion):\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.val_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def test_model(self, model):\n",
    "        \"\"\"Evaluate trained model performance on test dataset\"\"\"\n",
    "        model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        total_test_loss = 0.0\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_inputs, batch_labels in self.testing_loader:\n",
    "                batch_inputs, batch_labels = batch_inputs.to(self.device), batch_labels.to(self.device)\n",
    "                model_outputs = model(batch_inputs)\n",
    "                loss = loss_function(model_outputs, batch_labels)\n",
    "                \n",
    "                total_test_loss += loss.item()\n",
    "                _, predicted_classes = torch.max(model_outputs, 1)\n",
    "                total_samples += batch_labels.size(0)\n",
    "                correct_predictions += (predicted_classes == batch_labels).sum().item()\n",
    "        \n",
    "        average_test_loss = total_test_loss / len(self.testing_loader)\n",
    "        test_accuracy = 100. * correct_predictions / total_samples\n",
    "        return average_test_loss, test_accuracy\n",
    "    \n",
    "    def train_model(self, config, epochs=200, patience=20):\n",
    "        \"\"\"Train model with given hyperparameter configuration\"\"\"\n",
    "        \n",
    "        # Create model\n",
    "        model = create_resnet(\n",
    "            layers=config['layers'],\n",
    "            num_classes=100,\n",
    "            dropout_rate=config.get('dropout', 0.0)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Setup training components\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if config['optimizer'] == 'sgd':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=config['lr'], \n",
    "                                momentum=config.get('momentum', 0.9),\n",
    "                                weight_decay=config.get('weight_decay', 1e-4))\n",
    "        elif config['optimizer'] == 'adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config['lr'],\n",
    "                                 weight_decay=config.get('weight_decay', 1e-4))\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        if config.get('scheduler') == 'step':\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n",
    "        elif config.get('scheduler') == 'cosine':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        else:\n",
    "            scheduler = None\n",
    "        \n",
    "        # Training tracking\n",
    "        train_losses, train_accs = [], []\n",
    "        val_losses, val_accs = [], []\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"\\nTraining with config: {config}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_epoch(model, optimizer, criterion)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = self.validate_epoch(model, criterion)\n",
    "            \n",
    "            # Update scheduler\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Save metrics\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0 or epoch < 5:\n",
    "                print(f'Epoch {epoch+1}/{epochs} ({epoch_time:.1f}s)')\n",
    "                print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "                print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "                if scheduler:\n",
    "                    print(f'LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "                print(\"-\" * 40)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), f'best_model_{hash(str(config))}.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "        # Load best model for final evaluation\n",
    "        model.load_state_dict(torch.load(f'best_model_{hash(str(config))}.pth'))\n",
    "        \n",
    "        # Test on test set\n",
    "        test_loss, test_acc = self.test_model(model)\n",
    "        \n",
    "        results = {\n",
    "            'config': config,\n",
    "            'train_losses': train_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'final_test_loss': test_loss,\n",
    "            'final_test_acc': test_acc,\n",
    "            'epochs_trained': len(train_losses)\n",
    "        }\n",
    "        \n",
    "        print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "        print(f'Final test accuracy: {test_acc:.2f}%')\n",
    "        \n",
    "        return results, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c97b8b0-d4fd-44e2-808a-97aca0369fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search():\n",
    "    \"\"\"Perform hyperparameter search\"\"\"\n",
    "    \n",
    "    # Define hyperparameter configurations to test\n",
    "    configs = [\n",
    "        {\n",
    "            'name': 'Baseline SGD',\n",
    "            'layers': [2, 4, 4, 2],\n",
    "            'optimizer': 'sgd',\n",
    "            'lr': 0.1,\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 1e-4,\n",
    "            'scheduler': 'step',\n",
    "            'dropout': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'SGD with Dropout',\n",
    "            'layers': [2, 4, 4, 2],\n",
    "            'optimizer': 'sgd',\n",
    "            'lr': 0.1,\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 1e-4,\n",
    "            'scheduler': 'step',\n",
    "            'dropout': 0.2\n",
    "        },\n",
    "        {\n",
    "            'name': 'Adam Optimizer',\n",
    "            'layers': [2, 4, 4, 2],\n",
    "            'optimizer': 'adam',\n",
    "            'lr': 0.001,\n",
    "            'weight_decay': 1e-4,\n",
    "            'scheduler': 'cosine',\n",
    "            'dropout': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'Higher Learning Rate',\n",
    "            'layers': [2, 4, 4, 2],\n",
    "            'optimizer': 'sgd',\n",
    "            'lr': 0.2,\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 5e-4,\n",
    "            'scheduler': 'step',\n",
    "            'dropout': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'Deeper Network',\n",
    "            'layers': [3, 6, 6, 3],\n",
    "            'optimizer': 'sgd',\n",
    "            'lr': 0.1,\n",
    "            'momentum': 0.9,\n",
    "            'weight_decay': 1e-4,\n",
    "            'scheduler': 'step',\n",
    "            'dropout': 0.1\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    trainer = CIFAR100Trainer()\n",
    "    trainer.load_data(batch_size=128)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing configuration: {config['name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        results, model = trainer.train_model(config, epochs=200, patience=20)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a5f5b3-3c52-43b3-9656-72bd70551322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_list):\n",
    "    \"\"\"Plot training curves for multiple configurations\"\"\"\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ResNet Training Results on CIFAR-100', fontsize=16)\n",
    "    \n",
    "    # Plot training accuracy\n",
    "    ax1 = axes[0, 0]\n",
    "    for results in results_list:\n",
    "        epochs = range(1, len(results['train_accs']) + 1)\n",
    "        ax1.plot(epochs, results['train_accs'], \n",
    "                label=f\"{results['config']['name']}\", linewidth=2)\n",
    "    ax1.set_title('Training Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    ax2 = axes[0, 1]\n",
    "    for results in results_list:\n",
    "        epochs = range(1, len(results['val_accs']) + 1)\n",
    "        ax2.plot(epochs, results['val_accs'], \n",
    "                label=f\"{results['config']['name']}\", linewidth=2)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax3 = axes[1, 0]\n",
    "    for results in results_list:\n",
    "        epochs = range(1, len(results['train_losses']) + 1)\n",
    "        ax3.plot(epochs, results['train_losses'], \n",
    "                label=f\"{results['config']['name']}\", linewidth=2)\n",
    "    ax3.set_title('Training Loss')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot validation loss\n",
    "    ax4 = axes[1, 1]\n",
    "    for results in results_list:\n",
    "        epochs = range(1, len(results['val_losses']) + 1)\n",
    "        ax4.plot(epochs, results['val_losses'], \n",
    "                label=f\"{results['config']['name']}\", linewidth=2)\n",
    "    ax4.set_title('Validation Loss')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d389c7-42bf-4314-b2ef-2862b595e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_summary(results_list):\n",
    "    \"\"\"Print summary of all hyperparameter configurations\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER SEARCH RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print table header\n",
    "    print(f\"{'Configuration':<20} {'Best Val Acc':<12} {'Test Acc':<10} {'Test Loss':<10} {'Epochs':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Sort by test accuracy\n",
    "    sorted_results = sorted(results_list, key=lambda x: x['final_test_acc'], reverse=True)\n",
    "    \n",
    "    for results in sorted_results:\n",
    "        config_name = results['config']['name']\n",
    "        best_val = results['best_val_acc']\n",
    "        test_acc = results['final_test_acc']\n",
    "        test_loss = results['final_test_loss']\n",
    "        epochs = results['epochs_trained']\n",
    "        \n",
    "        print(f\"{config_name:<20} {best_val:<12.2f} {test_acc:<10.2f} {test_loss:<10.4f} {epochs:<8}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Best configuration details\n",
    "    best_config = sorted_results[0]\n",
    "    print(f\"\\nBEST CONFIGURATION: {best_config['config']['name']}\")\n",
    "    print(f\"Test Accuracy: {best_config['final_test_acc']:.2f}%\")\n",
    "    print(f\"Test Loss: {best_config['final_test_loss']:.4f}\")\n",
    "    print(f\"Configuration details:\")\n",
    "    for key, value in best_config['config'].items():\n",
    "        if key != 'name':\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f418db-c03a-42ea-ba84-8e437af0c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output_sizes():\n",
    "    \"\"\"Print output sizes for each layer\"\"\"\n",
    "    sizes = calculate_layer_output_dimensions()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LAYER OUTPUT SIZES\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Layer Name':<15} {'Output Size':<20} {'Description'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    descriptions = {\n",
    "        'input': 'Input images',\n",
    "        'conv1': 'Initial convolution',\n",
    "        'conv2_x': 'Residual blocks (no downsampling)',\n",
    "        'conv3_x': 'Residual blocks (downsample 2x)',\n",
    "        'conv4_x': 'Residual blocks (downsample 2x)',\n",
    "        'conv5_x': 'Residual blocks (downsample 2x)',\n",
    "        'avgpool': 'Global average pooling',\n",
    "        'fc': 'Final classification layer'\n",
    "    }\n",
    "    \n",
    "    for layer, size in sizes.items():\n",
    "        if len(size) == 3:\n",
    "            size_str = f\"({size[0]}, {size[1]}, {size[2]})\"\n",
    "        else:\n",
    "            size_str = f\"({size[0]},)\"\n",
    "        print(f\"{layer:<15} {size_str:<20} {descriptions[layer]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df6ed60c-8b17-440d-8168-c5972d11107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LAYER OUTPUT SIZES\n",
      "==================================================\n",
      "Layer Name      Output Size          Description\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Print network architecture details\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     print_output_sizes()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Set random seeds for reproducibility\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m, in \u001b[0;36mprint_output_sizes\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     size_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescriptions[layer]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_image'"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Print network architecture details\n",
    "    print_output_sizes()\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Run hyperparameter search\n",
    "    print(\"\\nStarting hyperparameter search...\")\n",
    "    all_results = hyperparameter_search()\n",
    "    \n",
    "    # Print summary\n",
    "    print_results_summary(all_results)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(all_results[:3])  # Plot first 3 configurations\n",
    "    \n",
    "    # Save results\n",
    "    with open('resnet_cifar100_results.json', 'w') as f:\n",
    "        # Convert to serializable format\n",
    "        serializable_results = []\n",
    "        for result in all_results:\n",
    "            serializable_result = result.copy()\n",
    "            # Convert numpy arrays to lists if any\n",
    "            for key in ['train_losses', 'train_accs', 'val_losses', 'val_accs']:\n",
    "                if key in serializable_result:\n",
    "                    serializable_result[key] = [float(x) for x in serializable_result[key]]\n",
    "            serializable_results.append(serializable_result)\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to 'resnet_cifar100_results.json'\")\n",
    "\n",
    "# Quick test to verify model construction\n",
    "def test_model_construction():\n",
    "    \"\"\"Test that the model can be constructed and forward pass works\"\"\"\n",
    "    print(\"\\nTesting model construction...\")\n",
    "    \n",
    "    model = create_resnet(layers=[2, 4, 4, 2], num_classes=100)\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(1, 3, 32, 32)\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"✓ Model construction successful!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment to test model construction\n",
    "# test_model_construction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84f34b-95e3-4f8c-bb5e-9eb32993c4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8db37-e8be-4b80-8b8c-f8d1405d7db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205af59-8ce0-4a51-8824-19ebb8d3e323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc2f85-47c9-40be-81e2-a7bc0309ac5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8c59c-da03-4f67-b808-27ae80fd0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hyperparameters(self, hyperparameter_config, max_epochs=200, early_stop_patience=20):\n",
    "        \"\"\"Train model using specified hyperparameter configuration\"\"\"\n",
    "        \n",
    "        # Build model with configuration\n",
    "        model = build_custom_resnet(\n",
    "            layer_structure=hyperparameter_config['layers'],\n",
    "            num_classes=100,\n",
    "            dropout_rate=hyperparameter_config.get('dropout', 0.0)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Setup loss function\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Setup optimizer based on configuration\n",
    "        if hyperparameter_config['optimizer'] == 'sgd':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=hyperparameter_config['lr'], \n",
    "                                momentum=hyperparameter_config.get('momentum', 0.9),\n",
    "                                weight_decay=hyperparameter_config.get('weight_decay', 1e-4))\n",
    "        elif hyperparameter_config['optimizer'] == 'adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=hyperparameter_config['lr'],\n",
    "                                 weight_decay=hyperparameter_config.get('weight_decay', 1e-4))\n",
    "        \n",
    "        # Setup learning rate scheduler\n",
    "        if hyperparameter_config.get('scheduler') == 'step':\n",
    "            lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n",
    "        elif hyperparameter_config.get('scheduler') ==import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Print network architecture details\n",
    "    print_output_sizes()\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Run hyperparameter search\n",
    "    print(\"\\nStarting hyperparameter search...\")\n",
    "    all_results = hyperparameter_search()\n",
    "    \n",
    "    # Print summary\n",
    "    print_results_summary(all_results)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(all_results[:3])  # Plot first 3 configurations\n",
    "    \n",
    "    # Save results\n",
    "    with open('resnet_cifar100_results.json', 'w') as f:\n",
    "        # Convert to serializable format\n",
    "        serializable_results = []\n",
    "        for result in all_results:\n",
    "            serializable_result = result.copy()\n",
    "            # Convert numpy arrays to lists if any\n",
    "            for key in ['train_losses', 'train_accs', 'val_losses', 'val_accs']:\n",
    "                if key in serializable_result:\n",
    "                    serializable_result[key] = [float(x) for x in serializable_result[key]]\n",
    "            serializable_results.append(serializable_result)\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to 'resnet_cifar100_results.json'\")\n",
    "\n",
    "# Quick test to verify model construction\n",
    "def test_model_construction():\n",
    "    \"\"\"Test that the model can be constructed and forward pass works\"\"\"\n",
    "    print(\"\\nTesting model construction...\")\n",
    "    \n",
    "    model = create_resnet(layers=[2, 4, 4, 2], num_classes=100)\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(1, 3, 32, 32)\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"✓ Model construction successful!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment to test model construction\n",
    "# test_model_construction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
