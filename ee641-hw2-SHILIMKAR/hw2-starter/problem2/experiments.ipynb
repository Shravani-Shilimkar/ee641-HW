{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import os\n",
        "\n",
        "# Make sure these imports match your file names\n",
        "from hierarchical_vae import HierarchicalDrumVAE\n",
        "from dataset import DrumPatternDataset\n",
        "\n",
        "# --- Configuration ---\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_PATH = \"results/best_model.pth\"\n",
        "DATA_DIR = \"data/drums\"\n",
        "RESULTS_DIR = \"results\"\n",
        "\n",
        "# --- Create Directories ---\n",
        "os.makedirs(os.path.join(RESULTS_DIR, \"generated_patterns\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(RESULTS_DIR, \"latent_analysis\"), exist_ok=True)\n",
        "\n",
        "# --- Load Model ---\n",
        "model = HierarchicalDrumVAE(z_high_dim=4, z_low_dim=12).to(DEVICE)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded successfully.\")\n",
        "\n",
        "# --- Load Dataset ---\n",
        "dataset = DrumPatternDataset(DATA_DIR, split='all')\n",
        "print(f\"âœ… Dataset with {len(dataset)} patterns loaded.\")\n",
        "\n",
        "# --- Helper function for plotting and saving ---\n",
        "def save_pattern(pattern, path, title=\"\"):\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    plt.imshow(pattern.T, aspect='auto', origin='lower', cmap='gray_r', interpolation='nearest')\n",
        "    plt.xlabel(\"Timestep\")\n",
        "    plt.ylabel(\"Instrument\")\n",
        "    plt.yticks(np.arange(9), dataset.instrument_names, fontsize=8)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path)\n",
        "    plt.close() # Close the plot to save memory"
      ],
      "metadata": {
        "id": "zQ868jyzJHDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MK110vbNJM9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸŽµ Generating 10 samples for each style...\")\n",
        "\n",
        "# Get a representative style vector (z_high) for each genre\n",
        "style_latents = {}\n",
        "with torch.no_grad():\n",
        "    for i, style_name in enumerate(dataset.style_names):\n",
        "        # Find the first pattern with this style index\n",
        "        pattern_idx = np.where(dataset.styles == i)[0][0]\n",
        "        pattern, _, _ = dataset[pattern_idx]\n",
        "\n",
        "        # Encode it to get the style vector\n",
        "        _, _, _, _, _, z_high = model.encode_hierarchy(pattern.unsqueeze(0).to(DEVICE))\n",
        "        style_latents[style_name] = z_high\n",
        "\n",
        "# Generate and save patterns\n",
        "for style_name, z_high_style in style_latents.items():\n",
        "    style_dir = os.path.join(RESULTS_DIR, \"generated_patterns\", style_name)\n",
        "    os.makedirs(style_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(10):\n",
        "            # Keep the style (z_high) fixed, but sample a new rhythm (z_low) from the prior\n",
        "            z_low_sample = torch.randn(1, model.z_low_dim).to(DEVICE)\n",
        "\n",
        "            # Decode to generate the pattern\n",
        "            logits = model.decode_hierarchy(z_high_style, z_low_sample)\n",
        "            generated_pattern = (torch.sigmoid(logits) > 0.5).float().squeeze(0).cpu().numpy()\n",
        "\n",
        "            # Save the resulting pattern image\n",
        "            save_path = os.path.join(style_dir, f\"sample_{i+1}.png\")\n",
        "            save_pattern(generated_pattern, save_path, title=f\"{style_name.capitalize()} Sample {i+1}\")\n",
        "\n",
        "print(\"âœ… Saved all generated style samples to 'results/generated_patterns/'\")"
      ],
      "metadata": {
        "id": "wnvb2mNpJMvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oG8UElX5JQ6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ”„ Generating interpolation sequence (Jazz to Rock)...\")\n",
        "\n",
        "# 1. Select a Jazz and a Rock pattern\n",
        "jazz_idx = np.where(dataset.styles == dataset.style_names.index('jazz'))[0][0]\n",
        "rock_idx = np.where(dataset.styles == dataset.style_names.index('rock'))[0][0]\n",
        "jazz_pattern, _, _ = dataset[jazz_idx]\n",
        "rock_pattern, _, _ = dataset[rock_idx]\n",
        "\n",
        "# 2. Encode them to get their latent vectors\n",
        "with torch.no_grad():\n",
        "    p1 = jazz_pattern.unsqueeze(0).to(DEVICE)\n",
        "    p2 = rock_pattern.unsqueeze(0).to(DEVICE)\n",
        "    _, _, z_low1, _, _, z_high1 = model.encode_hierarchy(p1)\n",
        "    _, _, z_low2, _, _, z_high2 = model.encode_hierarchy(p2)\n",
        "\n",
        "# 3. Interpolate, decode, and save each step\n",
        "interp_dir = os.path.join(RESULTS_DIR, \"generated_patterns\", \"interpolation_jazz_rock\")\n",
        "os.makedirs(interp_dir, exist_ok=True)\n",
        "n_steps = 10\n",
        "\n",
        "for i, alpha in enumerate(np.linspace(0, 1, n_steps)):\n",
        "    # Interpolate both style (z_high) and rhythm (z_low)\n",
        "    z_high_interp = (1 - alpha) * z_high1 + alpha * z_high2\n",
        "    z_low_interp = (1 - alpha) * z_low1 + alpha * z_low2\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model.decode_hierarchy(z_high_interp, z_low_interp)\n",
        "        recon_pattern = (torch.sigmoid(logits) > 0.5).float().squeeze(0).cpu().numpy()\n",
        "\n",
        "    save_path = os.path.join(interp_dir, f\"interp_step_{i}.png\")\n",
        "    save_pattern(recon_pattern, save_path, title=f\"Interpolation Step {i} (alpha={alpha:.2f})\")\n",
        "\n",
        "print(f\"âœ… Saved interpolation sequence to '{interp_dir}'\")"
      ],
      "metadata": {
        "id": "6I7LDTYoJRSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mScf-I15JTu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸŽ­ Generating style transfer example (Hip-hop rhythm -> Latin style)...\")\n",
        "\n",
        "# 1. Select a Hip-hop pattern (for rhythm) and a Latin pattern (for style)\n",
        "hiphop_idx = np.where(dataset.styles == dataset.style_names.index('hiphop'))[0][0]\n",
        "latin_idx = np.where(dataset.styles == dataset.style_names.index('latin'))[0][0]\n",
        "hiphop_pattern, _, _ = dataset[hiphop_idx]\n",
        "latin_pattern, _, _ = dataset[latin_idx]\n",
        "\n",
        "# 2. Encode both to get their respective latent vectors\n",
        "with torch.no_grad():\n",
        "    p_rhythm = hiphop_pattern.unsqueeze(0).to(DEVICE)\n",
        "    p_style = latin_pattern.unsqueeze(0).to(DEVICE)\n",
        "    # Get z_low from the hip-hop beat\n",
        "    _, _, z_low_rhythm, _, _, _ = model.encode_hierarchy(p_rhythm)\n",
        "    # Get z_high from the latin beat\n",
        "    _, _, _, _, _, z_high_style = model.encode_hierarchy(p_style)\n",
        "\n",
        "# 3. Combine them and decode\n",
        "with torch.no_grad():\n",
        "    logits = model.decode_hierarchy(z_high_style, z_low_rhythm)\n",
        "    style_transfer_pattern = (torch.sigmoid(logits) > 0.5).float().squeeze(0).cpu().numpy()\n",
        "\n",
        "# 4. Save all three patterns for comparison\n",
        "transfer_dir = os.path.join(RESULTS_DIR, \"generated_patterns\", \"style_transfer\")\n",
        "os.makedirs(transfer_dir, exist_ok=True)\n",
        "save_pattern(hiphop_pattern.numpy(), os.path.join(transfer_dir, \"01_source_rhythm_hiphop.png\"), \"Source Rhythm (Hip-hop)\")\n",
        "save_pattern(latin_pattern.numpy(), os.path.join(transfer_dir, \"02_source_style_latin.png\"), \"Source Style (Latin)\")\n",
        "save_pattern(style_transfer_pattern, os.path.join(transfer_dir, \"03_style_transfer_result.png\"), \"Result: Hip-hop Rhythm in Latin Style\")\n",
        "\n",
        "print(f\"âœ… Saved style transfer examples to '{transfer_dir}'\")"
      ],
      "metadata": {
        "id": "w-f5FkmVJUCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hl7jX5UGJW2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ”¬ Performing latent space analysis...\")\n",
        "\n",
        "# 1. t-SNE visualization (re-generated from analyze_latent.py)\n",
        "# This part is identical to the script to ensure the deliverable is created\n",
        "all_z_high = []\n",
        "all_styles = []\n",
        "with torch.no_grad():\n",
        "    for i in range(len(dataset)):\n",
        "        pattern, style, _ = dataset[i]\n",
        "        _, _, _, _, _, z_high = model.encode_hierarchy(pattern.unsqueeze(0).to(DEVICE))\n",
        "        all_z_high.append(z_high.cpu().numpy())\n",
        "        all_styles.append(style)\n",
        "\n",
        "all_z_high = np.concatenate(all_z_high, axis=0)\n",
        "all_styles = np.array(all_styles)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate='auto', init='pca')\n",
        "z_high_embedded = tsne.fit_transform(all_z_high)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(z_high_embedded[:, 0], z_high_embedded[:, 1], c=all_styles, cmap='viridis', alpha=0.7)\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=dataset.style_names, title=\"Styles\")\n",
        "plt.title(\"t-SNE Visualization of High-Level Latent Space (z_high)\")\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"latent_analysis\", \"tsne_z1_space.png\"))\n",
        "plt.close()\n",
        "print(\"âœ… Saved t-SNE plot.\")\n",
        "\n",
        "# 2. Dimension Interpretation / Disentanglement Analysis\n",
        "# We test how a single z_low dimension affects the output for a fixed style\n",
        "z_high_electronic = style_latents['electronic']\n",
        "z_low_base = torch.zeros(1, model.z_low_dim).to(DEVICE)\n",
        "dim_to_vary = 5 # You can experiment with changing this from 0 to 11\n",
        "n_steps = 7\n",
        "\n",
        "fig, axes = plt.subplots(1, n_steps, figsize=(15, 3))\n",
        "for i, val in enumerate(np.linspace(-2.5, 2.5, n_steps)):\n",
        "    z_low_varied = z_low_base.clone()\n",
        "    z_low_varied[0, dim_to_vary] = val\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model.decode_hierarchy(z_high_electronic, z_low_varied)\n",
        "        pattern = (torch.sigmoid(logits) > 0.5).float().squeeze(0).cpu().numpy()\n",
        "\n",
        "    density = np.sum(pattern)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(pattern.T, aspect='auto', origin='lower', cmap='gray_r')\n",
        "    ax.set_title(f\"Dim {dim_to_vary} = {val:.1f}\\nDensity={int(density)}\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "fig.suptitle(f\"Dimension Interpretation: Varying z_low[{dim_to_vary}] for Electronic Style\")\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"latent_analysis\", f\"dimension_interpretation_dim{dim_to_vary}.png\"))\n",
        "plt.close()\n",
        "print(\"âœ… Saved dimension interpretation plot.\")"
      ],
      "metadata": {
        "id": "oJx5OOPpJXMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEfahTgcJaLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lj5dgAsGJZ7i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}